# -*- coding: utf-8 -*-
"""import_graph_csv.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11t2WWfsPLNYVUFugb21mVluQ__ptXxbN
"""

#!pip install dgl
#!pip install BorutaShap
#!pip install umap-learn
#!pip install plotly
import pandas as pd
import numpy
import os
import sys
import random
numpy.set_printoptions(threshold=sys.maxsize)
pd.options.display.max_columns = None
from BorutaShap import BorutaShap
import dgl
from dgl.data import DGLDataset
import torch
torch.set_printoptions(edgeitems=10000)
from dgl.nn import GraphConv
from dgl.nn import GNNExplainer
import torch.nn as nn
import torch.nn.functional as F
from dgl.dataloading import GraphDataLoader
from torch.utils.data.sampler import SubsetRandomSampler
from umap import UMAP
import plotly.express as px


"""## Load graph data from CSV"""
epochs_graph=100
epochs_node=100

"""#Graph Classification"""

class MyDataset(DGLDataset):
    def __init__(self):
        super().__init__(name='custom_dataset')

    def process(self):
        self.num_classes=2
        self.graphs = []
        self.labels = []
        #node_features = torch.from_numpy(nodes_data[['X5','Y5']].to_numpy()).float()
        node_features = torch.from_numpy(nodes_data.to_numpy()).float()
        node_labels = torch.from_numpy(nodes_data['label'].to_numpy()).long()
        #edge_features = torch.from_numpy(edges_data['Weight'].to_numpy())
        edges_src = torch.from_numpy(edges_data['src'].to_numpy())
        edges_dst = torch.from_numpy(edges_data['dst'].to_numpy())

        self.graph = dgl.graph((edges_src, edges_dst), num_nodes=nodes_data.shape[0])#self_loop=True
        self.graph.ndata['feat'] = node_features
        self.graph.ndata['label'] = node_labels
        #self.graph.edata['weight'] = edge_features

        # If your dataset is a node classification dataset, you will need to assign
        # masks indicating whether a node belongs to training, validation, and test set.
        n_nodes = nodes_data.shape[0]
        n_train = int(n_nodes * 0.6)
        n_val = int(n_nodes * 0.2)
        train_mask = torch.zeros(n_nodes, dtype=torch.bool)
        val_mask = torch.zeros(n_nodes, dtype=torch.bool)
        test_mask = torch.zeros(n_nodes, dtype=torch.bool)
        train_mask[:n_train] = True
        val_mask[n_train:n_train + n_val] = True
        test_mask[n_train + n_val:] = True
        self.graph.ndata['train_mask'] = train_mask
        self.graph.ndata['val_mask'] = val_mask
        self.graph.ndata['test_mask'] = test_mask

        # Create a graph for each graph ID from the edges table.
        # First process the properties table into two dictionaries with graph IDs as keys.
        # The label and number of nodes are values.
        label_dict = {}
        num_nodes_dict = {}
        for _, row in properties_data.iterrows():
            label_dict[row['graph_id']] = row['label']
            num_nodes_dict[row['graph_id']] = row['num_nodes']

        # For the edges, first group the table by graph IDs.
        edges_group = edges_data.groupby('graph_id')
        nodes_group = nodes_data.groupby('graph_id')

        # For each graph ID...
        for graph_id in edges_group.groups:
            # Find the edges as well as the number of nodes and its label.
            edges_of_id = edges_group.get_group(graph_id)
            src = edges_of_id['src'].to_numpy()
            dst = edges_of_id['dst'].to_numpy()
            num_nodes = num_nodes_dict[graph_id]
            label = label_dict[graph_id]

            # Create a graph and add it to the list of graphs and labels.
            g = dgl.graph((src, dst), num_nodes=num_nodes)
            #g = dgl.graph((src, dst))
            nodes_of_id = nodes_group.get_group(graph_id)
            node_features = torch.from_numpy(nodes_of_id.drop(columns=['label']).to_numpy()).float()
            node_labels = torch.from_numpy(nodes_of_id['label'].to_numpy()).long()
            g.ndata['feat'] = node_features
            g.ndata['label'] = node_labels
            n_nodes = num_nodes
            n_train = int(n_nodes * 0.4)
            n_val = int(n_nodes * 0.1)
            train_mask = torch.zeros(n_nodes, dtype=torch.bool)
            val_mask = torch.zeros(n_nodes, dtype=torch.bool)
            test_mask = torch.zeros(n_nodes, dtype=torch.bool)
            train_mask[:n_train] = True
            val_mask[n_train:n_train + n_val] = True
            test_mask[n_train + n_val:] = True
            g.ndata['train_mask'] = train_mask
            g.ndata['val_mask'] = val_mask
            g.ndata['test_mask'] = test_mask
            g = dgl.add_self_loop(g)
            self.graphs.append(g)
            self.labels.append(label)
            )

        # Convert the label list to tensor for saving.
        self.labels = torch.LongTensor(self.labels)

    #def __getitem__(self, i):
    #    return self.graph

    #def __len__(self):
    #    return 1

    def __getitem__(self, i):
        return self.graphs[i], self.labels[i]

    def __len__(self):
        return len(self.graphs)

directory = './'+sys.argv[1]+"/"
#list_of_files = sorted( filter( lambda x: os.path.isfile(os.path.join(directory, x)),
#                        os.listdir(directory) ) )
'''
    For the given path, get the List of all files in the directory tree 
'''
def getListOfFiles(dirName):
    # create a list of file and sub directories 
    # names in the given directory 
    listOfFile = os.listdir(dirName)
    allFiles = list()
    # Iterate over all the entries
    for entry in listOfFile:
        # Create full path
        fullPath = os.path.join(dirName, entry)
        # If entry is a directory then get the list of files in this directory 
        if os.path.isdir(fullPath):
            allFiles = allFiles + getListOfFiles(fullPath)
        else:
            allFiles.append(fullPath)
                
    return allFiles

# Get the list of all files in directory tree at given path
list_of_files = getListOfFiles(directory)
random.shuffle(list_of_files)

#Train data
graphID = 0
nodes_data = pd.DataFrame()
edges_data = pd.DataFrame()
properties_data = pd.DataFrame()
temp_nodes_data = pd.DataFrame()
temp_edges_data = pd.DataFrame()
temp_properties_data = pd.DataFrame()

for entry in list_of_files:
       if entry.endswith('.csv') and "train" in entry:
        print(entry)
        subdirname = os.path.basename(os.path.dirname(entry))
        #print(subdirname)
        
        temp_data = pd.read_csv(entry)
        temp_data['graph_id'] = graphID
        #temp_data['label'] = int(subdirname)
        temp_data['name'] = entry
        #if temp_data.label[0] != -1:
        nodes_data = pd.concat([nodes_data, temp_data], axis="rows", ignore_index=True)
        #temp_properties_data = temp_data[:1][['graph_id','Target']]
        temp_properties_data = temp_data[:1][['graph_id','label','name']]
        temp_properties_data['num_nodes']=temp_data.shape[0]
        properties_data = pd.concat([properties_data, temp_properties_data], axis="rows", ignore_index=True)
        #properties_data=properties_data[:1]
        graphID=graphID+1
        temp_edges_data = temp_data[['graph_id']]
        temp_edges_data.insert(1, "src", 0)
        temp_edges_data.insert(1, "dst", 1)
        for index, row in temp_data.iterrows():
         temp_edges_data.iloc[index,1]=index
         temp_edges_data.iloc[index,2]=index+1
        temp_edges_data.drop(temp_edges_data.tail(1).index,inplace=True)
        edges_data = pd.concat([edges_data, temp_edges_data], axis="rows", ignore_index=True)
nodes_data = nodes_data.select_dtypes(['number'])

train_dataset = MyDataset()
g, label = train_dataset[:]
print(g, label)

#Test data
#graphID = 0
nodes_data = pd.DataFrame()
edges_data = pd.DataFrame()
#properties_data = pd.DataFrame()
temp_nodes_data = pd.DataFrame()
temp_edges_data = pd.DataFrame()
temp_properties_data = pd.DataFrame()

for entry in list_of_files:
       if entry.endswith('.csv') and "test" in entry:
        print(entry)
        subdirname = os.path.basename(os.path.dirname(entry))
        #print(subdirname)
        
        temp_data = pd.read_csv(entry)
        temp_data['graph_id'] = graphID
        #temp_data['label'] = int(subdirname)
        temp_data['name'] = entry
        #if temp_data.label[0] != -1:
        nodes_data = pd.concat([nodes_data, temp_data], axis="rows", ignore_index=True)
        #temp_properties_data = temp_data[:1][['graph_id','Target']]
        temp_properties_data = temp_data[:1][['graph_id','label','name']]
        temp_properties_data['num_nodes']=temp_data.shape[0]
        properties_data = pd.concat([properties_data, temp_properties_data], axis="rows", ignore_index=True)
        #properties_data=properties_data[:1]
        graphID=graphID+1
        temp_edges_data = temp_data[['graph_id']]
        temp_edges_data.insert(1, "src", 0)
        temp_edges_data.insert(1, "dst", 1)
        for index, row in temp_data.iterrows():
         temp_edges_data.iloc[index,1]=index
         temp_edges_data.iloc[index,2]=index+1
        temp_edges_data.drop(temp_edges_data.tail(1).index,inplace=True)
        edges_data = pd.concat([edges_data, temp_edges_data], axis="rows", ignore_index=True)
nodes_data = nodes_data.select_dtypes(['number'])

test_dataset = MyDataset()
g, label = test_dataset[:]
print(g, label)

print(properties_data)

num_examples = len(train_dataset)
num_train = int(num_examples * 1)

#train_sampler = SubsetRandomSampler(torch.arange(num_train))
#test_sampler = SubsetRandomSampler(torch.arange(num_train, num_examples))

train_dataloader = GraphDataLoader(
    #train_dataset, sampler=train_sampler, batch_size=1, drop_last=False, shuffle=True)
    train_dataset, batch_size=1, drop_last=False, shuffle=True)
test_dataloader = GraphDataLoader(
    test_dataset, batch_size=1, drop_last=False, shuffle=False)
    #test_dataset, sampler=test_sampler, batch_size=1, drop_last=False, shuffle=False)

'''
# If no model is selected default is the Random Forest
# If classification is True it is a classification problem
Feature_Selector = BorutaShap(importance_measure='shap', classification=True)

temp_nodes_data = nodes_data.sample(frac = 0.1)
Feature_Selector.fit(X=temp_nodes_data.drop(columns=['label','graph_id']), y=temp_nodes_data['label'], n_trials=100, random_state=0)
##Feature_Selector.fit(X=nodes_data.drop(columns=['label','Timestamp','Sample','graph_id']), y=nodes_data['label'], n_trials=50, random_state=0)

Feature_Selector.plot(which_features='accepted', figsize=(16,12))

nodes_data = nodes_data.drop(columns=Feature_Selector.rejected)

#df = px.data.iris()
df = nodes_data

#features = df.loc[:, :'petal_width']
labels = nodes_data['label']
features = nodes_data.drop(columns=['label'])
#features = nodes_data[new_nodes_data]

umap_2d = UMAP(n_components=2, init='spectral', random_state=0, n_neighbors=50, n_epochs=100, densmap=False, set_op_mix_ratio=0.25, metric='euclidean')
umap_3d = UMAP(n_components=3, init='spectral', random_state=0, n_neighbors=50, n_epochs=100, densmap=False, set_op_mix_ratio=0.25, metric='euclidean')

proj_2d = umap_2d.fit_transform(features,labels)
proj_3d = umap_3d.fit_transform(features,labels)

fig_2d = px.scatter(
    proj_2d, x=0, y=1,
    color=df.label, labels={'color': 'label'}
)
fig_3d = px.scatter_3d(
    proj_3d, x=0, y=1, z=2,
    color=df.label, labels={'color': 'label'}
)
fig_3d.update_traces(marker_size=5)

fig_2d.show()
fig_3d.show()
'''

#it = iter(train_dataloader)
#batch = next(it)
#print(batch)

#batched_graph, labels = batch
#print('Number of nodes for each graph element in the batch:', batched_graph.batch_num_nodes())
#print('Number of edges for each graph element in the batch:', batched_graph.batch_num_edges())

# Recover the original graph elements from the minibatch
#graphs = dgl.unbatch(batched_graph)
#print('The original graphs in the minibatch:')
#print(graphs)

from dgl.nn import GraphConv

class GCN(nn.Module):
    #def __init__(self, in_feats, h_feats, num_classes):
    def __init__(self, feat, h_feats, num_classes):
        super(GCN, self).__init__()
        self.conv1 = GraphConv(feat, h_feats)
        self.conv2 = GraphConv(h_feats, num_classes)
    
    #def forward(self, g, in_feat):
    def forward(self, graph, feat, eweight=None):
        h = self.conv1(graph, feat)
        h = F.relu(h)
        h = self.conv2(graph, h)
        graph.ndata['h'] = h
        return dgl.mean_nodes(graph, 'h')

dataset = MyDataset()
g, label = dataset[0]
print(g, label)

model = GCN(g.ndata['feat'].shape[1], g.num_nodes(), dataset.num_classes)

optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

for epoch in range(epochs_graph):
    #print("Epoch: ", epoch)
    for batched_graph, labels in train_dataloader:
        pred = model(batched_graph, batched_graph.ndata['feat'].float())
        loss = F.cross_entropy(pred, labels)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

num_correct = 0
num_tests = 0
for batched_graph, labels in test_dataloader:
    pred = model(batched_graph, batched_graph.ndata['feat'].float())
    #print(batched_graph)
    print("Label: ", labels)
    print("Prediction: ", pred.argmax(1), "\n")
    num_correct += (pred.argmax(1) == labels).sum().item()
    num_tests += len(labels)

print("num_examples:",  num_examples)
print("num_train:",  num_train)
print('Test accuracy:', num_correct / num_tests)

explainer = GNNExplainer(model, num_hops=50, lr=0.001, num_epochs=200, alpha1=0.01, alpha2=2.0, beta1=2.0, beta2=0.5, log=True)
dataset = MyDataset()
g, _ = dataset[0]
features = g.ndata['feat']
feat_mask, edge_mask = explainer.explain_graph(g, features)
print(g)
print(feat_mask)
print(edge_mask)

g, _ = dataset[1]
features = g.ndata['feat']
feat_mask, edge_mask = explainer.explain_graph(g, features)
print(g)
print(feat_mask)
print(edge_mask)

"""#Node Classification"""

class GCN(nn.Module):
    def __init__(self, in_feats, h_feats, num_classes):
        super(GCN, self).__init__()
        self.conv1 = GraphConv(in_feats, h_feats)
        self.conv2 = GraphConv(h_feats, num_classes)

    def forward(self, g, in_feat):
        h = self.conv1(g, in_feat)
        h = F.relu(h)
        h = self.conv2(g, h)
        return h
    
# Create the model with given dimensions

#dataset = CoraGraphDataset()
#g = dataset[0]
#print(g)

dataset = MyDataset()
g, label = dataset[0]

model = GCN(g.ndata['feat'].shape[1], g.num_nodes(), dataset.num_classes)

optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
#best_val_acc = 0
#best_test_acc = 0

#features = g.ndata['feat']
#labels = g.ndata['label']
#train_mask = g.ndata['train_mask']
#val_mask = g.ndata['val_mask']
#test_mask = g.ndata['test_mask']
for e in range(epochs_node):
 for batched_graph, labels in train_dataloader:
    # Forward
    logits = model(batched_graph, batched_graph.ndata['feat'])
    #logits = model(g, features)

    # Compute prediction
    pred = logits.argmax(1)

    # Compute loss
    # Note that you should only compute the losses of the nodes in the training set.
    loss = F.cross_entropy(logits, batched_graph.ndata['label'])
    #loss = F.cross_entropy(logits[train_mask], labels[train_mask])

    # Compute accuracy on training/validation/test
    #train_acc = (pred[train_mask] == labels[train_mask]).float().mean()
    #val_acc = (pred[val_mask] == labels[val_mask]).float().mean()
    #test_acc = (pred[test_mask] == labels[test_mask]).float().mean()

    # Save the best validation accuracy and the corresponding test accuracy.
    #if best_val_acc < val_acc:
    #    best_val_acc = val_acc
    #    best_test_acc = test_acc


    # Backward
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    #if e % 5 == 0:
    #    print('In epoch {}, loss: {:.3f}, val acc: {:.3f} (best {:.3f}), test acc: {:.3f} (best {:.3f})'.format(
    #        e, loss, val_acc, best_val_acc, test_acc, best_test_acc))


num_correct = 0
num_tests = 0
for batched_graph, labels in test_dataloader:
    print(batched_graph)
    pred = model(batched_graph, batched_graph.ndata['feat'].float())
    print("Label: ", batched_graph.ndata['label'])
    print("Prediction: ", pred.argmax(1), "\n")
    #num_correct += (pred.argmax(1) == labels).sum().item()
    num_correct += (pred.argmax(1) == batched_graph.ndata['label']).sum().item()
    #num_tests += len(labels)
    num_tests += len(batched_graph.ndata['label'])
print('Number Correct:', num_correct)
print('Number Tests:', num_tests)
print('Test accuracy:', num_correct / num_tests)

# importing libraries
import cv2
import numpy as np
 
# Create a VideoCapture object and read from input file
cap = cv2.VideoCapture(
    './Tim/test/0/IMG_0569.MOV', cv2.CAP_FFMPEG)
fps = cap.get(cv2.CAP_PROP_FPS)
print("Frame rate: ", int(fps), "FPS")

# Check if file opened successfully
if (cap.isOpened()== False):
    print("Error opening video file")
frame_number=0
counter=0
# We need to set resolutions.
# so, convert them from float to integer.
frame_width = int(cap.get(3))
frame_height = int(cap.get(4))
   
size = (frame_width, frame_height)
   
# Below VideoWriter object will create
# a frame of above defined The output 
# is stored in 'filename.avi' file.
result = cv2.VideoWriter('./Tim/test/0/PREDICTED_GOOD_IMG_0569.avi', 
                         cv2.VideoWriter_fourcc(*'MJPG'),
                         10, size)
# Read until video is completed
while(cap.isOpened()):
 # Capture frame-by-frame
    ret, frame = cap.read()
    if ret == True:
    # Display the resulting frame
       if(frame_number % 3 == 0):
        if pred.argmax(1)[counter]==1: 
        #if batched_graph.ndata['label'][counter]==1:
         result.write(frame)
         cv2.imshow('Frame', frame)
        counter+=1
         
    # Press Q on keyboard to exit
       if cv2.waitKey(25) & 0xFF == ord('q'):
         break
 
# Break the loop
    else:
        break
    frame_number+=1     
# When everything done, release
# the video capture object
# When everything done, release 
# the video capture and video 
# write objects
cap.release()
result.release()
 
# Closes all the frames
cv2.destroyAllWindows()
